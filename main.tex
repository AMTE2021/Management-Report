\documentclass{article}
\usepackage[utf8]{inputenc}

\usepackage{a4wide}


\title{AMTE21 - Management report}
\author{Patrick Diehl \\ Irina Demeshko \\ Zhara Khatami \\ Steven R. Brandt \\ Parsa Amini}
\date{\today}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Key indicators}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The first submission deadline was May 7th with an extension to May 14th. We received three full papers at the first extension. Note that the workshop does not accept short papers. The deadline was extended to June 14th, and we received eight submissions at the final deadline. In total, eight papers were submitted on June 14th and five papers were accepted ($62.5$\%). The workshop was scheduled at X, and we had an average attendance of x persons. The keynote was given by Thomas Fahringer (University of Innsbruck, Austria) and x persons attended the keynote. The invited talks was given by Daisy Hollman (Google) and x persons attended. The workshops closed with a panel discussion, and x persons attended the panel discussion. More details are available on the workshop's webpage\footnote{https://amte2021.stellar-group.org/}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Committees}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Organizing committee}
\label{sec:committee}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
    \item Irina Demeshko, Los Alamos National Laboratory, USA
    \item Patrick Diehl, Center for Computation \& Technology at Louisiana State University, USA
    \item Zahra Khatami, NVIDIA, USA
    \item Steven R. Brandt, Center for Computation \& Technology at Louisiana State University, USA
    \item Parsa Amini, Center for Computation \& Technology at Louisiana State University, USA
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Program committee}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note that we tried to have a good mixture between European-based and American-based program committee members.

\begin{itemize}
    \item Metin H. Aktulga, Michigan State University, USA
    \item Bryce Adelstein Lelbach, NVIDIA, USA
    \item John Biddiscombe, Swiss National Supercomputing Centre, Switzerland
    \item Gregor Daiss, University of Stuttgart, Germany
    \item Vassilios Dimakopoulos, University of Ioannina, Greece
    \item Patricia Grubel, Los Alamos National Laboratory, USA
    \item Jeff Hammond, NVIDIA, USA
    \item Adrian Lemoine, AMD, USA
    \item Roman Lakymchuk, Fraunhofer ITWM, Germany
    \item Thomas Heller, Exasol, Germany
    \item Kevin Huck, University of Oregon, USA
    \item Daisy Hollman, Sandia National Laboratories, USA
    \item Laxmikant (Sanjay) V. Kale, University of Illinois at Urbana-Champaign, USA
    \item Hartmut Kaiser, Center for Computation \& Technology at Louisiana State University, USA
    \item Erwin Laure, Max Planck Computing \& Data Facility, Germany
    \item Andrew Lumsdaine, Northwest Institute for Advanced Computing, USA
    \item Pat McCormick, Los Alamos National Laboratory, USA
    \item Dirk Pleiter, Jülich Supercomputing Centre, Germany
    \item Brad Richardson, Sourcery Institute, USA
    \item Galen Shipman, Los Alamos National Laboratory, USA
    \item Mikael Simberg, Swiss National Supercomputing Centre, Switzerland
    \item Shahrzad Shirzad, Center for Computation \& Technology at Louisiana State University, USA
    \item Sean Treichler, NVIDIA, USA
    \item Didem Unat, Koç University, Turkey
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reviewers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In addition to the organizers these program committee members reviewed at least one paper:
\begin{itemize}
\item Hartmut Kaiser, Center for Computation \& Technology at Louisiana State University, USA
\item Didem Unat, Koç University, Turkey
\item Dirk Pleiter, Jülich Supercomputing Centre, Germany
\item Roman Lakymchuk, Fraunhofer ITWM, Germany
\item Vassilios Dimakopoulos, University of Ioannina, Greece
\item Thomas Heller, Exasol, Germany
\item Jeff Hammond, NVIDIA
\item Dirk Pleiter, Jülich Supercomputing Centre, Germany
\item Hartmut Kaiser, Center for Computation \& Technology at Louisiana State University, USA
\item Erwin Laure, Max Planck Computing \& Data Facility, Germany
\item Gregor Daiss, University of Stuttgart, Germany
\item Vassilios Dimakopoulos, University of Ioannina, Greece
\item Patricia Grubel, Los Alamos National Laboratory, USA
\item Jeff Hammond, NVIDIA, USA
\item Adrian Lemoine, AMD, USA
\item Roman Lakymchuk, Fraunhofer ITWM, Germany
item  Mikael Simberg, Swiss National Supercomputing Centre, Switzerland
\item Laxmikant (Sanjay) V. Kale, University of Illinois at Urbana-Champaign, USA
\item Galen Shipman, Los Alamos National Laboratory, USA
\item Brad Richardson, Sourcery Institute, USA
\item Shahrzad Shirzad, Center for Computation \& Technology at Louisiana State University, USA
\end{itemize}
The reviewers were assigned to the papers based on their expertise and avoiding any conflict of interest. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Review process management}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Each paper was reviewed by at least four independent reviewers selected from the initial program committee using EasyChair. Note that all reviewers are listed in Section~\ref{sec:committee}. The deadline for the reviewers was set to June 25th. However, we received the last missing review on June 29th. The organization committee discussed the one boarder line paper via email and made their final decision of accepted paper on June 29th. The final notification was sent to the authors using EasyChair on June 30th. The revised paper was requested by August 1st combined with the \LaTeX sources of the paper. All papers were uploaded to iThenticate for plagiarism check and the report was provided to the submitting author. The authors were asked to address the highlighted issues in the Ithenticate report and the reviewer’s comments before the submission of the camera ready version. 


\section{Porgramm}

\subsection{Talks}

\begin{itemize}
    \item 9:00am - 9:05 am, Opening Remarks
    \item 9:05am - 10:05 am, Keynote Talk (Thomas Fahringer, UIBK)
    \item10:05am - 10:20am, Morning Break
    \item 10:20am - 10:50am Selected paper talk
    \item 10:50am - 11:20am, Selected paper talk
    \item 11:20am - 11:50am, Selected paper talk
    \item 11:50am - 12:20pm, Selected paper talk
    \item 12:20pm - 1:30pm, Lunch Break
    \item 1:30pm - 2:30pm, Invited talk (Daisy S. Hollman, Sandia National Laboratory, USA)
    \item 2:30pm - 3:00pm, Selected paper talk
    \item 3:00pm - 3:30pm, Selected paper talk
    \item 3:30pm -3:45 pm Afternoon break
    \item 3:45pm - 4:45pm, Panel
\end{itemize}


\subsection{Panel}

Moderator: Irina Demeshko, Los Alamos National Laboratory, USA\\

Panelists:

\begin{itemize}
    \item Hartmut Kaiser, Center for Computation \& Technology at Louisiana State University, USA
    \item  Laxmikant (Sanjay) Kale, Department of Computer Science, University of Illinois at Urbana-Champaign, USA
    \item  Martin Berzins, Scientific Computing and Imaging Institute, University of Utah, USA
    \item  Mike Bauer, NVIDIA, USA
    \item Thomas Fahringer, University of Innsbruck, Austria
\end{itemize}

Panel's chosen questions:

\begin{itemize}
    \item 
    Why should one choose to use AMT models?
    \item
    What kinds of applications will benefit from using AMT models and programming systems?
    \item
    For what kinds of applications is AMT-style programming better suited to creating maintainable code than message passing-style code?
    Are AMT programs more difficult to debug because it is (1) easier to create race conditions and (2) more difficult to get anything like a stack trace? What is being done to make debugging of large-scale AMT applications easier?
    \item
    What sort of language support is needed for AMT programming?
    \begin{itemize}
        \item  What advantage do newer languages like Go, Julia, etc., have for AMT programming, and can/should they displace C++ for scientific applications?
        \item Many languages have added “await” or some variant as new keywords for processing asynchronous code. How important is this feature for the success of the AMT paradigm?
    \end{itemize}
    \item
    How to choose which AMT programming model among others?
\end{itemize}

\section{List of accpeted papers}

\begin{enumerate}
    \item Accelerating Messages by Avoiding Copies in an Asynchronous Task-based Programming Model \\
    Authors: Nitin Bhat, Sam White and Laxmikant V Kale 
    \begin{center}
        \textbf{Abstract}
    \end{center}
    Communication is critical to the scalable and efficient performance of scientific simulations on extreme scale computing systems. Part of the promise of task-based programming models is that they can naturally overlap communication with c    omputation and exploit locality between tasks. Copy based semantics using eager communication protocols easily enable such asynchrony by alleviating the responsibility of buffer management from the user, both on the sender and the receiver. However, these semantics increase memory allocations and copies and in turn affect application memory footprint and performance, especially with large messages. In this work we extend the Charm++ programming model with so-called ``zero copy'' messaging semantics. These semantics work on user-provided buffers and do not semantically require copies by either the user or the runtime system. This ena    bles in-place data movement of various kinds. We motivate our work by reviewing the existing messaging models supported by Charm++, identify their semantic shortcomings with respect to large message handling, and define new APIs for communication based on RDMA capabilities. We demonstrate the utility of our new communication interfaces by the use of benchmarks written in Charm++ and AMPI. The result is up to 90\% of message latency improvement and nearly 2x lower peak mem    ory usage.
    \item Combining OpenMP tasking and target (GPU) offloading for productivity and performance on heterogeneous systems \\
    Authors: Pedro Valero-Lara, Jungwon Kim, Oscar Hernandez and Jeffrey Vetter
    \begin{center}
        \textbf{Abstract}
    \end{center}
    We evaluate the use of OpenMP tasking with target (GPU) offloading as a potential solution for programming productivity and performance on heterogeneous systems. As test case, We use one of the most popular and widely used BLAS Level-3 routines, triangular solver(TRSM). To benefit from the heterogeneity of the current HPC systems, we propose a different parallelization of the algorithm by using a non uniform decomposition of the problem. We use target (GPU) offloading inside OpenMP tasks to deal with the heterogeneity found in the hardware. This new approach is able to outperform the state-of-the-art algorithms, which use a uniform decomposition of the data, on both: CPU only and CPU+GPU systems, reaching speedups of up to one order of magnitude. The performance achieved by this approach is faster than the IBM ESSL math library on CPU and competitive w.r.t. a highly optimized heterogeneous CUDA version. For the performance analysis, we have used one node of the ORNL’s supercomputer Summit.
    \item FleCSI 2.0: The Flexible Computational Science Infrastructure Project\\
    Authors: Ben Bergen, Irina Demeshko, Charles Ferenbaugh, Davis Herring, Li-Ta Lo, Julien Loiseau, Navamita Ray and Andrew Reisner
    \begin{center}
        \textbf{Abstract}
    \end{center}
    The FleCSI 2.0 programming system supports multiphysics application development through a runtime abstraction layer, and by providing core topology types that can be customized for speciﬁc nu-merical methods. The abstraction layer provides a single-source programming interface for distributed and shared-memory data parallelism through task and kernel execution, and has been demonstrated to introduce virtually no runtime overhead. FleCSI’s core topology types represent a rich set of basic data structures that can be specialized to create application-facing interfaces for a variety of different physics packages. Using the FleCSI control and data models, it is straightforward to compose multiple packages to create full multiphysics applications. When used with the Legion backend, FleCSI offers extended runtime analysis that can increase task concurrency, facilitate load balancing, and allow for portability across heterogeneous computing architectures.
    \item An Experimental Study of SYCL Task Graph Parallelism for Large-Scale Machine Learning Workloads\\
    Authors: Cheng-Hsiang Chiu, Dian-Lun Lin and Tsung-Wei Huang
    \begin{center}
        \textbf{Abstract}
    \end{center}
    Task graph parallelism has emerged as an important tool to efficiently execute large machine learning workloads on GPUs. Users describe a GPU workload in a task dependency graph rather than aggregated GPU operations and dependencies, allowing the runtime to run whole-graph scheduling optimization to significantly improve the performance. While the new CUDA graph execution model has demonstrated significant success on this front, the counterpart for SYCL, a general-purpose heterogeneous programming model using standard C++, remains nascent. Unlike CUDA graph, SYCL runtime leverages out-of-order queues to implicitly create a task execution graph induced by data dependencies. For explicit task dependencies, users are responsible for creating SYCL events and synchronizing them at a non-negligible cost. Furthermore, there is no specialized graph execution model that allows users to offload a task graph directly onto a SYCL device similarly to CUDA graph. This paper conducts an experimental study of SYCL’s default task graph parallelism by comparing it with CUDA graph on large-scale machine learning workloads in the recent HPEC Graph Challenge. Our result highlights the need for a new SYCL graph
execution model in the standard.
\item Understanding the Effect of Task Granularity on Execution Time in Asynchronous Many-Task Runtime Systems\\
Authors: Shahrzad Shirzad, Rod Tohid, Alireza Kheirkhahan, Bibek Wagle and Hartmut Kaiser
    \begin{center}
        \textbf{Abstract}
    \end{center}
    Task granularity is a key factor in determining the performance of asynchronous many-task (AMT) runtime systems. The over-head of scheduling an excessive number of tasks with smaller granularities causes performance degradation while creating a few larger tasks leads to starvation and therefore under-utilization of resources. In this paper, we developed an analytical model of the execution time of an application with balanced parallel for-loops in terms of grain size, and number of cores. The parameters of this model mostly depend on the runtime and the architecture. We introduce an approach to suggest a range of possible grain sizes to achieve the best performance based on the proposed model. To the best of our knowledge, our analytical model is the first to explain the relationship between the execution time in terms of grain size, runtime, and physical characteristics of the machine in an asynchronous runtime system.
\end{enumerate}



\end{document}
